\section{Motivation}
Synthesizing program code from natural language is a challenging task as natural
language utterances tend to be ambiguous and require substantial prior knowledge
to interpret. Recent solutions approach these difficulties in different ways. Some models do not
constrain their inputs and operate with a large variety of sentences, but tend
to be less accurate. Others limit the space of possible inputs, requiring them
to meet a fixed structure which makes them more similar to code than language.

This paper offers a middle ground between these approaches. We train a
transition-based neural network on descriptions of programming tasks generated by
using context-free grammar and templates. We show that the model is able to
generalize and can solve synthesis problems described in natural language.
The approach could be very useful in a wide variety of program synthesis problems where natural language data is scarce.