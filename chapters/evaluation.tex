\section{Evaluation}

In this section, we discuss the datasets created and the results obtained. We
have two datasets: one that consists of the (structured task description, Python
code snippet) pairs for training, and one that has unstructured natural language
task descriptions for testing.

\subsection{Datasets}

This section describes the two datasets introduced above. In order to test how
the model generalizes to unconstrained natural language, the datasets have been
created with different methods and priorities in mind.

The first dataset is used for training TranX and is generated by the process
described in \cref{sec:STDT}. This dataset has been created by using templates so
the emphasis is on the structure over anything else. Such generated utterances
do not have to be perfectly correct either syntactically or semantically. The
template fields have to be easily recognizable which is required to produce the
code snippets. The dataset contains \( 42000 \) examples: \( 6 \) algorithmic
patterns each with \( 7 \) templates and each template has \( 1000 \) examples.

The second dataset, which is handmade, contains unconstrained natural language
task descriptions and is used for testing. This dataset prioritizes semantic and
syntactic correctness. Still, all information is included in the sentences that
is needed to solve the task (i.e., create the Python code snippet) and if
possible the order of relevant tokens matches the algorithm rule. This dataset
contains \( 60 \) examples, 10 examples each for the six algorithmic patterns.

\subsection{Results}

In this section, the results are presented. The Corpus BLEU\cite{bleu} metric is
used, a standard metric when it comes to comparing sequences of strings. The
authors of TranX also used this metric to measure its performance on Python 3
output~\cite{tranxRepo}. We compare our results with their results of TranX on
the Conala\cite{conala} dataset. This dataset does not use any predefined
structure.

First, we tested TranX on the generated dataset it has been trained on. It has
achieved very good results with a Corpus BLEU score of 0.98. This is not
surprising since our templates have been relatively simple and due to the nature
of the generated dataset, the grammatical structure is often repeated, which
makes it easier to distinguish the template fields from the extra tokens.

Achieving high scores on our generated dataset is certainly good, but the real
test is how the model performs on more realistic sentences. In order to test
that, we used the second, handmade dataset, which has semantically and
syntactically correct natural language sentences while it also contains all the
information needed to synthesize the corresponding programs.

\cref{tab:result} shows that TranX was able to generalize to natural language
task descriptions despite the fact that it is trained on inputs generated by
templates. It achieved a relatively good BLEU score compared to the results of
the Conala dataset which does not contain any predefined structure.

\begin{table}[h]
  \normalsize \centering
  \begin{tabular}{lrl}
    \toprule
    Dataset  & Model & Metric \\
    \midrule
    Generated &  0.98  & Corpus BLEU \\
    Handmade  &  0.43  & Corpus BLEU \\
    Conala    &  0.245 & Corpus BLEU \\
    \bottomrule
  \end{tabular}
  \vspace{0.1cm}
  \caption{Model Corpus BLEU score on different datasets.}
  \label{tab:result}
\end{table}

\subsection{Discussion}

Getting good results on the generated dataset has been expected because of the
fixed structure of our training input sentences. Templates have ensured that the
important tokens are in predictable positions. Adding more templates to each
algorithm has diversified our dataset and has allowed the nonterminals of the
algorithm rule to appear in different locations in the structured task
descriptions. Despite this, overfitting on sentences created by templates is a
real possibility and has to be avoided if we want to process natural task
descriptions.

The handmade dataset has been created to test if TranX has learned to generalize
from structured to natural language task descriptions. TranX has been able to
reach considerably better results on this handmade dataset than on Conala. This
means that the model must be capable of some level of generalization and
recognize our CFG-grammar in a natural language environment.

It is also clear that TranX has performed worse on the handmade dataset compared
to the generated dataset. However, the handmade dataset contains more complex
examples with structures that TranX has not seen during training.

It is very likely that our results have been caused by two major factors. First,
the fixed structure of our inputs have made our utterances more regular hence
more recognizable for TranX. The second factor could have been the cardinality
of our dataset. Since by using our method it is easy to generate large datasets,
neural networks can work with more data and see more examples.